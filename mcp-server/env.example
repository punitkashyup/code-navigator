# ========================================
# RAG MCP SERVER CONFIGURATION
# ========================================

# ========================================
# OPENSEARCH CONFIGURATION
# ========================================
# OpenSearch connection URL (AWS OpenSearch Service or local)
OPENSEARCH_URL=https://your-opensearch-domain.us-east-1.es.amazonaws.com
# OpenSearch authentication
OPENSEARCH_USER=admin
OPENSEARCH_ADMIN_PW=YourSecurePassword123!
# OpenSearch index and field configuration
OPENSEARCH_INDEX=code_chunks
OPENSEARCH_TEXT_FIELD=text
OPENSEARCH_VECTOR_FIELD=vector_field
OPENSEARCH_TIMEOUT=60

# ========================================
# EMBEDDING MODEL CONFIGURATION
# Choose between "bedrock" (recommended) or "openai"
# ========================================
EMBEDDING_PROVIDER=bedrock

# --- BEDROCK EMBEDDINGS (Primary Choice) ---
# AWS credentials for Bedrock access
AWS_ACCESS_KEY_ID=your_access_key_here
AWS_SECRET_ACCESS_KEY=your_secret_key_here
AWS_SESSION_TOKEN=your_session_token_here_if_using_temporary_credentials
AWS_REGION=us-east-1

# Bedrock embedding model configuration
BEDROCK_EMBEDDING_MODEL=amazon.titan-embed-text-v2:0
# Alternative models:
# BEDROCK_EMBEDDING_MODEL=amazon.titan-embed-text-v1
# BEDROCK_EMBEDDING_MODEL=cohere.embed-english-v3
# BEDROCK_EMBEDDING_MODEL=cohere.embed-multilingual-v3

# Connection pool settings for better performance
BEDROCK_MAX_POOL_CONNECTIONS=50

# --- OPENAI EMBEDDINGS (Fallback Option) ---
# Only needed if EMBEDDING_PROVIDER=openai or as fallback
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_EMBEDDING_MODEL=text-embedding-3-large
# Alternative models:
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small
# OPENAI_EMBEDDING_MODEL=text-embedding-ada-002

# ========================================
# LLM CONFIGURATION FOR RAG PIPELINE
# Choose between "openai" or "bedrock"
# ========================================
LLM_PROVIDER=openai

# --- OPENAI LLM SETTINGS (Recommended for RAG) ---
# Query expansion model (generates additional search queries)
QUERY_EXPANSION_MODEL=gpt-4o-mini
# Reranking model (ranks search results by relevance)
RERANKER_MODEL=gpt-4o

# --- BEDROCK LLM SETTINGS (Alternative) ---
# Only needed if LLM_PROVIDER=bedrock
BEDROCK_QUERY_EXPANSION_MODEL=anthropic.claude-3-haiku-20240307-v1:0
BEDROCK_RERANKER_MODEL=anthropic.claude-3-sonnet-20240229-v1:0
# Alternative models:
# BEDROCK_QUERY_EXPANSION_MODEL=anthropic.claude-3-5-sonnet-20241022-v2:0
# BEDROCK_RERANKER_MODEL=anthropic.claude-3-5-sonnet-20241022-v2:0

# ========================================
# RAG PIPELINE PARAMETERS
# Fine-tune these based on your needs
# ========================================

# Query expansion: Number of additional search queries to generate
QUERY_EXPANSION_N=3

# Search result limits
BM25_K=5                    # Number of results from keyword search per query
VECTOR_SEARCH_K=5           # Number of results from semantic search per query

# Final result limit
RERANK_TOP_M=5              # Number of chunks to return after reranking
MAX_CHUNKS=100              # Maximum chunks for metadata operations

# ========================================
# SERVER SETTINGS
# ========================================
HOST=0.0.0.0
PORT=8080
LOG_LEVEL=info

# MCP Server API Key for authentication
MCP_API_KEY=1234

# ========================================
# DEPLOYMENT NOTES
# ========================================

# For AWS OpenSearch Service:
# 1. Make sure your AWS credentials have proper OpenSearch permissions:
#    - es:ESHttpGet, es:ESHttpPost, es:ESHttpPut, es:ESHttpDelete
# 2. Update security group to allow connections from your server
# 3. Use the full OpenSearch domain endpoint URL

# For Bedrock:
# 1. Ensure your AWS credentials have Bedrock permissions:
#    - bedrock:InvokeModel for the embedding and LLM models you want to use
# 2. Make sure the models are available in your chosen AWS region
# 3. Some models may require explicit access requests

# For Local Development:
# 1. You can run OpenSearch locally using Docker:
#    docker run -p 9200:9200 -e "discovery.type=single-node" opensearchproject/opensearch:latest
# 2. Update OPENSEARCH_URL to http://localhost:9200
# 3. Use default credentials (admin/admin) or configure as needed

# Performance Tips:
# 1. Increase BEDROCK_MAX_POOL_CONNECTIONS if you have high concurrent load
# 2. Adjust BM25_K and VECTOR_SEARCH_K based on your corpus size
# 3. Tune RERANK_TOP_M based on how many results you want to return
# 4. For large deployments, consider using AWS Application Load Balancer 